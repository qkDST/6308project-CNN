# 1. å¯¼å…¥
import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,
                             roc_auc_score, confusion_matrix)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import (Input, Embedding, Conv1D, BatchNormalization,
                                     ReLU, MaxPooling1D, GlobalMaxPooling1D, Dense,
                                     Dropout, Concatenate)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import nltk

# ä¸‹è½½å¿…è¦èµ„æºï¼ˆä»…åˆ†è¯å’Œåœç”¨è¯åº“ï¼‰
nltk.download('punkt')
nltk.download('stopwords')

# 2. é…ç½®å‚æ•°
class Config:
    # æ ¸å¿ƒï¼šé…ç½®Yahooå’ŒYelpæ•°æ®é›†è·¯å¾„
    YAHOO_TRAIN_PATH = "C:/Users/31278/Desktop/text_classification/yahoo_answers_csv/train.csv"
    YAHOO_TEST_PATH = "C:/Users/31278/Desktop/text_classification/yahoo_answers_csv/test.csv"
    YELP_TRAIN_PATH = "C:/Users/31278/Desktop/text_classification/yelp_review_polarity_csv/train.csv"
    YELP_TEST_PATH = "C:/Users/31278/Desktop/text_classification/yelp_review_polarity_csv/test.csv"
    SAVE_MODEL_PATH = "best_models/"  # æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
    
    # æ•°æ®é¢„å¤„ç†å‚æ•°ï¼ˆåµŒå…¥å±‚éšæœºåˆå§‹åŒ–ï¼‰
    SAMPLE_RATIO = 0.2  # å–æ ·20%æ•°æ®ï¼ˆé¿å…è®­ç»ƒè¿‡ä¹…ï¼Œå¯æ”¹0.1æ›´å¿«ï¼‰
    MAX_VOCAB_SIZE = 50000  # è¯è¡¨æœ€å¤§å®¹é‡ï¼ˆå‰5ä¸‡é«˜é¢‘è¯ï¼‰
    YAHOO_MAX_LEN = 300  # Yahooé•¿æ–‡æœ¬åºåˆ—é•¿åº¦
    YELP_MAX_LEN = 200  # YelpçŸ­æ–‡æœ¬åºåˆ—é•¿åº¦
    EMBEDDING_DIM = 100  # åµŒå…¥å±‚ç»´åº¦ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE = 64  # é™ä½æ‰¹æ¬¡å¤§å°ï¼Œé¿å…å†…å­˜ä¸è¶³ï¼ˆæ ¹æ®ç”µè„‘æ€§èƒ½æ›¿æ¢64æˆ–128ï¼‰
    EPOCHS = 15  # å‡å°‘è®­ç»ƒè½®æ¬¡ï¼ŒåŠ å¿«é€Ÿåº¦
    LEARNING_RATE = 0.001
    PATIENCE = 3  # æ—©åœï¼š3è½®æ— æå‡åˆ™åœæ­¢

# åˆ›å»ºæ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹ï¼ˆä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
os.makedirs(Config.SAVE_MODEL_PATH, exist_ok=True)
config = Config()

# 3. æ•°æ®é¢„å¤„ç†å·¥å…·å‡½æ•°
def load_yahoo_data(train_path, test_path):
    """åŠ è½½Yahoo Answersæ•°æ®é›†ï¼ˆå¤šåˆ†ç±»ï¼š10ç±»ï¼‰"""
    # è¯»å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆé€‚é…æ–°Kaggleæ•°æ®é›†çš„åˆ—é¡ºåºï¼šcategoryâ†’titleâ†’contentâ†’answerï¼‰
    train_df = pd.read_csv(train_path, header=None, names=['category', 'title', 'content', 'answer'])
    test_df = pd.read_csv(test_path, header=None, names=['category', 'title', 'content', 'answer'])
    # åˆå¹¶åç»Ÿä¸€é¢„å¤„ç†ï¼ˆé¿å…åˆ†å¼€å¤„ç†å¯¼è‡´å·®å¼‚ï¼‰
    df = pd.concat([train_df, test_df], ignore_index=True)
    # è¿‡æ»¤ç©ºå€¼ï¼ˆé¿å…é¢„å¤„ç†æŠ¥é”™ï¼‰
    df = df.dropna(subset=['title', 'content', 'answer', 'category'])
    # æ‹¼æ¥æ–‡æœ¬ï¼ˆæ ‡é¢˜+å†…å®¹+å›ç­”ï¼Œä¿ç•™å®Œæ•´è¯­ä¹‰ï¼‰
    df['text'] = df['title'] + " " + df['content'] + " " + df['answer']
    # å–æ ·20%ï¼ˆåˆ†å±‚å–æ ·ï¼Œä¿æŒç±»åˆ«åˆ†å¸ƒå‡åŒ€ï¼‰
    df_sample, _ = train_test_split(df, test_size=1-config.SAMPLE_RATIO, 
                                    stratify=df['category'], random_state=42)
    # æ ‡ç­¾ç¼–ç ï¼ˆæ–‡æœ¬ç±»åˆ«â†’æ•´æ•°0-9ï¼Œé€‚é…æ¨¡å‹è¾“å‡ºï¼‰
    label_encoder = LabelEncoder()
    df_sample['label'] = label_encoder.fit_transform(df_sample['category'])
    return df_sample['text'].values, df_sample['label'].values, len(label_encoder.classes_)

def load_yelp_data(train_path, test_path):
    """åŠ è½½Yelp Review Polarityæ•°æ®é›†ï¼ˆäºŒåˆ†ç±»ï¼šæ­£è´Ÿè¯„è®ºï¼‰"""
    # è¯»å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆåˆ—é¡ºåºï¼šlabelâ†’textï¼‰
    train_df = pd.read_csv(train_path, header=None, names=['label', 'text'])
    test_df = pd.read_csv(test_path, header=None, names=['label', 'text'])
    # åˆå¹¶åç»Ÿä¸€é¢„å¤„ç†
    df = pd.concat([train_df, test_df], ignore_index=True)
    # è¿‡æ»¤ç©ºå€¼
    df = df.dropna(subset=['text'])
    # å–æ ·20%ï¼ˆåˆ†å±‚å–æ ·ï¼‰
    df_sample, _ = train_test_split(df, test_size=1-config.SAMPLE_RATIO, 
                                    stratify=df['label'], random_state=42)
    # æ ‡ç­¾è½¬æ¢ï¼ˆ1â†’0è´Ÿé¢ï¼Œ2â†’1æ­£é¢ï¼Œé€‚é…äºŒåˆ†ç±»æŸå¤±å‡½æ•°ï¼‰
    df_sample['label'] = df_sample['label'].map({1:0, 2:1})
    return df_sample['text'].values, df_sample['label'].values, 2

def clean_text(text):
    """æ–‡æœ¬æ¸…æ´—ï¼šå°å†™åŒ–+å»é™¤ç‰¹æ®Šå­—ç¬¦+å»å†—ä½™ç©ºæ ¼ï¼ˆå°ç™½æ— éœ€ä¿®æ”¹ï¼‰"""
    text = text.lower()  # ç»Ÿä¸€å°å†™ï¼ˆé¿å…å¤§å°å†™é‡å¤è®¡ç®—ï¼‰
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # åªä¿ç•™å­—æ¯å’Œç©ºæ ¼ï¼ˆå»é™¤æ ‡ç‚¹ã€æ•°å­—ç­‰ï¼‰
    text = re.sub(r'\s+', ' ', text).strip()  # å»é™¤å¤šä½™ç©ºæ ¼
    return text

def text_preprocess(texts, max_len, tokenizer=None, fit_tokenizer=True):
    """æ–‡æœ¬åºåˆ—åŒ–ï¼šåˆ†è¯â†’è¿‡æ»¤åœç”¨è¯â†’åºåˆ—è½¬æ¢â†’å¯¹é½"""
    stop_words = set(stopwords.words('english'))  # åŠ è½½è‹±æ–‡åœç”¨è¯ï¼ˆæ¯”å¦‚theã€aç­‰æ— æ„ä¹‰è¯ï¼‰
    tokenized_texts = []
    for text in texts:
        cleaned_text = clean_text(text)  # å…ˆæ¸…æ´—æ–‡æœ¬
        tokens = word_tokenize(cleaned_text)  # åˆ†è¯ï¼ˆæŠŠå¥å­æ‹†æˆå•ä¸ªå•è¯ï¼‰
        filtered_tokens = [token for token in tokens if token not in stop_words]  # è¿‡æ»¤åœç”¨è¯
        tokenized_texts.append(filtered_tokens)
    
    # æ„å»º/ä½¿ç”¨è¯è¡¨ï¼ˆæŠŠå•è¯â†’æ•´æ•°ç´¢å¼•ï¼‰
    if fit_tokenizer:
        tokenizer = Tokenizer(num_words=config.MAX_VOCAB_SIZE, oov_token='<OOV>')  # OOVï¼šæœªç™»å½•è¯æ ‡è®°
        tokenizer.fit_on_texts(tokenized_texts)  # åŸºäºè®­ç»ƒé›†æ„å»ºè¯è¡¨
    
    # æ–‡æœ¬â†’æ•´æ•°åºåˆ—
    sequences = tokenizer.texts_to_sequences(tokenized_texts)
    # åºåˆ—å¯¹é½ï¼ˆç»Ÿä¸€é•¿åº¦ï¼šé•¿æˆªæ–­ã€çŸ­å¡«å……ï¼‰
    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')
    return padded_sequences, tokenizer

# 4. æ¨¡å‹å®ç°ï¼ˆåµŒå…¥çŸ©é˜µï¼Œç”¨éšæœºåˆå§‹åŒ–åµŒå…¥å±‚ï¼‰
def build_very_deep_cnn(input_len, num_classes):
    """æ„å»ºVery Deep CNNæ¨¡å‹ï¼ˆæ— GloVeï¼ŒåµŒå…¥å±‚éšæœºåˆå§‹åŒ–ï¼‰"""
    # è¾“å…¥å±‚ï¼š(åºåˆ—é•¿åº¦,)
    inputs = Input(shape=(input_len,), name="input_layer")
    
    # åµŒå…¥å±‚ï¼šéšæœºåˆå§‹åŒ–
    embedding = Embedding(
        input_dim=config.MAX_VOCAB_SIZE + 1,  # è¯è¡¨å¤§å°+1ï¼ˆé¢„ç•™ç´¢å¼•0ï¼‰
        output_dim=config.EMBEDDING_DIM,     # åµŒå…¥ç»´åº¦ï¼ˆ100ç»´ï¼‰
        input_length=input_len,              # è¾“å…¥åºåˆ—é•¿åº¦
        trainable=True,                      # è®­ç»ƒä¸­å¯æ›´æ–°ï¼ˆé€‚é…ä»»åŠ¡æ•°æ®ï¼‰
        name="embedding_layer"
    )(inputs)
    
    # å·ç§¯å—1ï¼š3-gramï¼ˆæ•æ‰çŸ­è·ç¦»è¯­ä¹‰ï¼‰
    x = Conv1D(filters=64, kernel_size=3, padding='same', name="conv_3gram")(embedding)
    x = BatchNormalization(name="bn1")(x)  # æ‰¹é‡å½’ä¸€åŒ–ï¼šåŠ é€Ÿæ”¶æ•›
    x = ReLU(name="relu1")(x)              # æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§
    x = MaxPooling1D(pool_size=2, strides=1, padding='same', name="pool1")(x)  # æ± åŒ–ï¼šä¿ç•™å…³é”®ç‰¹å¾
    
    # å·ç§¯å—2ï¼š5-gramï¼ˆæ•æ‰ä¸­è·ç¦»è¯­ä¹‰ï¼‰
    x = Conv1D(filters=64, kernel_size=5, padding='same', name="conv_5gram")(x)
    x = BatchNormalization(name="bn2")(x)
    x = ReLU(name="relu2")(x)
    x = MaxPooling1D(pool_size=2, strides=1, padding='same', name="pool2")(x)
    
    # å·ç§¯å—3ï¼š3-gramï¼ˆå¢å¼ºçŸ­è·ç¦»ç‰¹å¾ï¼‰
    x = Conv1D(filters=64, kernel_size=3, padding='same', name="conv_3gram_2")(x)
    x = BatchNormalization(name="bn3")(x)
    x = ReLU(name="relu3")(x)
    x = MaxPooling1D(pool_size=2, strides=1, padding='same', name="pool3")(x)
    
    # å·ç§¯å—4ï¼š7-gramï¼ˆæ•æ‰é•¿è·ç¦»è¯­ä¹‰ï¼Œé€‚é…Yahooé•¿æ–‡æœ¬ï¼‰
    x = Conv1D(filters=64, kernel_size=7, padding='same', name="conv_7gram")(x)
    x = BatchNormalization(name="bn4")(x)
    x = ReLU(name="relu4")(x)
    
    # å…¨å±€æœ€å¤§æ± åŒ–ï¼šå°†å˜é•¿åºåˆ—â†’å›ºå®šé•¿åº¦å‘é‡
    x = GlobalMaxPooling1D(name="global_pool")(x)
    x = Dropout(0.5, name="dropout")(x)  # Dropoutï¼šç¼“è§£è¿‡æ‹Ÿåˆ
    
    # è¾“å‡ºå±‚ï¼šå¤šåˆ†ç±»ç”¨softmaxï¼ŒäºŒåˆ†ç±»ç”¨sigmoid
    activation = 'softmax' if num_classes > 2 else 'sigmoid'
    outputs = Dense(num_classes, activation=activation, name="output_layer")(x)
    
    model = Model(inputs=inputs, outputs=outputs, name="Very_Deep_CNN")
    return model

def dense_block(x, num_layers, growth_rate, block_name):
    """DenseNetå¯†é›†å—ï¼šç‰¹å¾å¤ç”¨ï¼ˆå°ç™½æ— éœ€ç†è§£ï¼Œç›´æ¥ä½¿ç”¨ï¼‰"""
    features = [x]
    for i in range(num_layers):
        # ç“¶é¢ˆå±‚ï¼š1x1å·ç§¯é™ç»´ï¼Œå‡å°‘è®¡ç®—é‡
        bottleneck = Conv1D(
            filters=4 * growth_rate,
            kernel_size=1,
            padding='same',
            name=f"{block_name}_bottleneck_{i}"
        )(Concatenate(name=f"{block_name}_concat_{i}")(features))
        bottleneck = BatchNormalization(name=f"{block_name}_bn_bottleneck_{i}")(bottleneck)
        bottleneck = ReLU(name=f"{block_name}_relu_bottleneck_{i}")(bottleneck)
        
        # ç‰¹å¾æå–å±‚ï¼š3x1å·ç§¯
        conv = Conv1D(
            filters=growth_rate,
            kernel_size=3,
            padding='same',
            name=f"{block_name}_conv_{i}"
        )(bottleneck)
        conv = BatchNormalization(name=f"{block_name}_bn_conv_{i}")(conv)
        conv = ReLU(name=f"{block_name}_relu_conv_{i}")(conv)
        
        features.append(conv)  # æ–°å¢ç‰¹å¾åŠ å…¥å¤ç”¨åˆ—è¡¨
    
    return Concatenate(name=f"{block_name}_final_concat")(features)

def transition_layer(x, compression, layer_name):
    """DenseNetè¿‡æ¸¡å±‚ï¼šå‹ç¼©ç‰¹å¾ç»´åº¦"""
    num_features = x.shape[-1]
    x = Conv1D(
        filters=int(num_features * compression),
        kernel_size=1,
        padding='same',
        name=f"{layer_name}_conv"
    )(x)
    x = BatchNormalization(name=f"{layer_name}_bn")(x)
    x = ReLU(name=f"{layer_name}_relu")(x)
    x = MaxPooling1D(pool_size=2, padding='same', name=f"{layer_name}_pool")(x)
    return x

def build_text_densenet(input_len, num_classes):
    """æ„å»ºText DenseNetæ¨¡å‹ï¼ˆåµŒå…¥å±‚éšæœºåˆå§‹åŒ–ï¼‰"""
    # è¾“å…¥å±‚
    inputs = Input(shape=(input_len,), name="input_layer")
    
    # åµŒå…¥å±‚ï¼šéšæœºåˆå§‹åŒ–
    embedding = Embedding(
        input_dim=config.MAX_VOCAB_SIZE + 1,
        output_dim=config.EMBEDDING_DIM,
        input_length=input_len,
        trainable=True,
        name="embedding_layer"
    )(inputs)
    
    # åˆå§‹å·ç§¯å±‚ï¼šå°†åµŒå…¥å‘é‡â†’ç‰¹å¾å›¾
    x = Conv1D(filters=32, kernel_size=3, padding='same', name="init_conv")(embedding)
    x = BatchNormalization(name="init_bn")(x)
    x = ReLU(name="init_relu")(x)
    
    # å¯†é›†å—1 + è¿‡æ¸¡å±‚1
    x = dense_block(x, num_layers=3, growth_rate=16, block_name="dense_block1")
    x = transition_layer(x, compression=0.5, layer_name="transition1")
    
    # å¯†é›†å—2 + è¿‡æ¸¡å±‚2
    x = dense_block(x, num_layers=3, growth_rate=16, block_name="dense_block2")
    x = transition_layer(x, compression=0.5, layer_name="transition2")
    
    # å…¨å±€æ± åŒ– + Dropout
    x = GlobalMaxPooling1D(name="global_pool")(x)
    x = Dropout(0.5, name="dropout")(x)
    
    # è¾“å‡ºå±‚
    activation = 'softmax' if num_classes > 2 else 'sigmoid'
    outputs = Dense(num_classes, activation=activation, name="output_layer")(x)
    
    model = Model(inputs=inputs, outputs=outputs, name="Text_DenseNet")
    return model

# 5. è®­ç»ƒä¸è¯„ä¼°å·¥å…·å‡½æ•°
def train_model(model, X_train, y_train, X_val, y_val, num_classes, model_name):
    """è®­ç»ƒæ¨¡å‹ï¼šå«æ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ã€ä¿å­˜æœ€ä½³æ¨¡å‹"""
    # é€‰æ‹©æŸå¤±å‡½æ•°ï¼šå¤šåˆ†ç±»â†’ç¨€ç–äº¤å‰ç†µï¼ŒäºŒåˆ†ç±»â†’äºŒå…ƒäº¤å‰ç†µ
    loss_fn = 'sparse_categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy'
    
    # ç¼–è¯‘æ¨¡å‹ï¼ˆä¼˜åŒ–å™¨ç”¨Adamï¼Œadamwå¦è®ºï¼‰
    model.compile(
        optimizer=Adam(learning_rate=config.LEARNING_RATE),
        loss=loss_fn,
        metrics=['accuracy']  # è®­ç»ƒæ—¶ç›‘æ§å‡†ç¡®ç‡
    )
    
    # æ—©åœï¼šé¿å…è¿‡æ‹Ÿåˆï¼ˆ3è½®éªŒè¯æŸå¤±æ— æå‡åˆ™åœæ­¢ï¼‰
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=config.PATIENCE,
        restore_best_weights=True,
        verbose=1
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦ï¼šéªŒè¯æŸå¤±åœæ»æ—¶ï¼Œå­¦ä¹ ç‡å‡åŠ
    lr_scheduler = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=2,
        min_lr=1e-6,  # æœ€å°å­¦ä¹ ç‡ï¼ˆé¿å…è¿‡å°å¯¼è‡´ä¸æ”¶æ•›ï¼‰
        verbose=1
    )
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰éªŒè¯å‡†ç¡®ç‡ï¼‰
    model_checkpoint = ModelCheckpoint(
        filepath=os.path.join(config.SAVE_MODEL_PATH, f"{model_name}_best.h5"),
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
    
    # å¼€å§‹è®­ç»ƒ
    history = model.fit(
        X_train, y_train,
        batch_size=config.BATCH_SIZE,
        epochs=config.EPOCHS,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, lr_scheduler, model_checkpoint],
        shuffle=True  # è®­ç»ƒé›†æ‰“ä¹±ï¼Œæå‡æ³›åŒ–èƒ½åŠ›
    )
    return history

def evaluate_model(model, X_test, y_test, num_classes, model_name, dataset_name):
    """è¯„ä¼°æ¨¡å‹ï¼šè®¡ç®—æ ¸å¿ƒæŒ‡æ ‡+å¯è§†åŒ–æ··æ·†çŸ©é˜µ"""
    # é¢„æµ‹ç»“æœ
    y_pred_proba = model.predict(X_test, verbose=0)  # é¢„æµ‹æ¦‚ç‡
    if num_classes > 2:
        y_pred = np.argmax(y_pred_proba, axis=1)  # å¤šåˆ†ç±»ï¼šå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
        # å¤šåˆ†ç±»æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='macro', zero_division=0
        )
        micro_prec, micro_rec, micro_f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='micro', zero_division=0
        )
        # æ‰“å°ç»“æœï¼ˆå¯ç›´æ¥çœ‹å‡†ç¡®ç‡å’ŒF1ï¼‰
        print(f"\nã€{dataset_name} - {model_name} å¤šåˆ†ç±»è¯„ä¼°ç»“æœã€‘")
        print(f"å‡†ç¡®ç‡ï¼š{accuracy:.4f}")
        print(f"å®å¹³å‡F1ï¼š{macro_f1:.4f}")
        print(f"å¾®å¹³å‡F1ï¼š{micro_f1:.4f}")
    else:
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()  # äºŒåˆ†ç±»ï¼šé˜ˆå€¼0.5
        # äºŒåˆ†ç±»æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        prec, rec, f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='binary', zero_division=0
        )
        auc = roc_auc_score(y_test, y_pred_proba)
        # æ‰“å°ç»“æœ
        print(f"\nã€{dataset_name} - {model_name} äºŒåˆ†ç±»è¯„ä¼°ç»“æœã€‘")
        print(f"å‡†ç¡®ç‡ï¼š{accuracy:.4f}")
        print(f"F1åˆ†æ•°ï¼š{f1:.4f}")
        print(f"AUCï¼š{auc:.4f}")
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=[f"ç±»{i}" for i in range(num_classes)],
                yticklabels=[f"ç±»{i}" for i in range(num_classes)])
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title(f"{dataset_name} - {model_name} æ··æ·†çŸ©é˜µ")
    plt.savefig(f"{model_name}_{dataset_name}_confusion_matrix.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    return accuracy

def plot_training_history(history, model_name, dataset_name):
    """å¯è§†åŒ–è®­ç»ƒå†å²ï¼šæŸå¤±+å‡†ç¡®ç‡æ›²çº¿ï¼ˆåˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¶æ•›ï¼‰"""
    plt.figure(figsize=(12, 4))
    
    # æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
    plt.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
    plt.xlabel('è½®æ¬¡ï¼ˆEpochï¼‰')
    plt.ylabel('æŸå¤±ï¼ˆLossï¼‰')
    plt.title(f"{dataset_name} - {model_name} æŸå¤±æ›²çº¿")
    plt.legend()
    
    # å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(history.history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
    plt.xlabel('è½®æ¬¡ï¼ˆEpochï¼‰')
    plt.ylabel('å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰')
    plt.title(f"{dataset_name} - {model_name} å‡†ç¡®ç‡æ›²çº¿")
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(f"{model_name}_{dataset_name}_training_history.png", dpi=300, bbox_inches='tight')
    plt.show()

# 6. ä¸»å‡½æ•°ï¼ˆæ‰§è¡Œå…¨æµç¨‹ï¼šåŠ è½½æ•°æ®â†’é¢„å¤„ç†â†’è®­ç»ƒâ†’è¯„ä¼°ï¼‰
def main():
    # æ­¥éª¤1ï¼šåŠ è½½Yahooæ•°æ®é›†
    print("="*50)
    print("1. åŠ è½½Yahoo Answersæ•°æ®é›†...")
    yahoo_texts, yahoo_labels, yahoo_num_classes = load_yahoo_data(
        config.YAHOO_TRAIN_PATH, config.YAHOO_TEST_PATH
    )
    # æ–‡æœ¬åºåˆ—åŒ–ï¼ˆè®­ç»ƒé›†æ„å»ºè¯è¡¨ï¼‰
    yahoo_sequences, yahoo_tokenizer = text_preprocess(
        yahoo_texts, max_len=config.YAHOO_MAX_LEN, fit_tokenizer=True
    )
    # åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼ˆ8:1:1ï¼‰
    X_train_yahoo, X_temp_yahoo, y_train_yahoo, y_temp_yahoo = train_test_split(
        yahoo_sequences, yahoo_labels, test_size=0.2, stratify=yahoo_labels, random_state=42
    )
    X_val_yahoo, X_test_yahoo, y_val_yahoo, y_test_yahoo = train_test_split(
        X_temp_yahoo, y_temp_yahoo, test_size=0.5, stratify=y_temp_yahoo, random_state=42
    )
    print(f"Yahooæ•°æ®é›†å‡†å¤‡å®Œæˆï¼šè®­ç»ƒé›†{len(X_train_yahoo)}æ¡ï¼ŒéªŒè¯é›†{len(X_val_yahoo)}æ¡ï¼Œæµ‹è¯•é›†{len(X_test_yahoo)}æ¡")

    # æ­¥éª¤2ï¼šåŠ è½½Yelpæ•°æ®é›†
    print("\n" + "="*50)
    print("2. åŠ è½½Yelp Review Polarityæ•°æ®é›†...")
    yelp_texts, yelp_labels, yelp_num_classes = load_yelp_data(
        config.YELP_TRAIN_PATH, config.YELP_TEST_PATH
    )
    # æ–‡æœ¬åºåˆ—åŒ–
    yelp_sequences, yelp_tokenizer = text_preprocess(
        yelp_texts, max_len=config.YELP_MAX_LEN, fit_tokenizer=True
    )
    # åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼ˆ8:1:1ï¼‰
    X_train_yelp, X_temp_yelp, y_train_yelp, y_temp_yelp = train_test_split(
        yelp_sequences, yelp_labels, test_size=0.2, stratify=yelp_labels, random_state=42
    )
    X_val_yelp, X_test_yelp, y_val_yelp, y_test_yelp = train_test_split(
        X_temp_yelp, y_temp_yelp, test_size=0.5, stratify=y_temp_yelp, random_state=42
    )
    print(f"Yelpæ•°æ®é›†å‡†å¤‡å®Œæˆï¼šè®­ç»ƒé›†{len(X_train_yelp)}æ¡ï¼ŒéªŒè¯é›†{len(X_val_yelp)}æ¡ï¼Œæµ‹è¯•é›†{len(X_test_yelp)}æ¡")

    # æ­¥éª¤3ï¼šè®­ç»ƒVery Deep CNNæ¨¡å‹
    print("\n" + "="*50)
    print("3. è®­ç»ƒVery Deep CNNæ¨¡å‹ï¼ˆYahooæ•°æ®é›†ï¼‰...")
    vdcnn_yahoo = build_very_deep_cnn(
        input_len=config.YAHOO_MAX_LEN,
        num_classes=yahoo_num_classes
    )
    vdcnn_yahoo_history = train_model(
        vdcnn_yahoo, X_train_yahoo, y_train_yahoo, X_val_yahoo, y_val_yahoo,
        num_classes=yahoo_num_classes, model_name="Very_Deep_CNN_Yahoo"
    )
    # è¯„ä¼°æ¨¡å‹
    evaluate_model(vdcnn_yahoo, X_test_yahoo, y_test_yahoo, yahoo_num_classes, "Very_Deep_CNN", "Yahoo")
    # å¯è§†åŒ–è®­ç»ƒå†å²
    plot_training_history(vdcnn_yahoo_history, "Very_Deep_CNN", "Yahoo")

    print("\n" + "="*50)
    print("4. è®­ç»ƒVery Deep CNNæ¨¡å‹ï¼ˆYelpæ•°æ®é›†ï¼‰...")
    vdcnn_yelp = build_very_deep_cnn(
        input_len=config.YELP_MAX_LEN,
        num_classes=yelp_num_classes
    )
    vdcnn_yelp_history = train_model(
        vdcnn_yelp, X_train_yelp, y_train_yelp, X_val_yelp, y_val_yelp,
        num_classes=yelp_num_classes, model_name="Very_Deep_CNN_Yelp"
    )
    evaluate_model(vdcnn_yelp, X_test_yelp, y_test_yelp, yelp_num_classes, "Very_Deep_CNN", "Yelp")
    plot_training_history(vdcnn_yelp_history, "Very_Deep_CNN", "Yelp")

    # æ­¥éª¤4ï¼šè®­ç»ƒText DenseNetæ¨¡å‹
    print("\n" + "="*50)
    print("5. è®­ç»ƒText DenseNetæ¨¡å‹ï¼ˆYahooæ•°æ®é›†ï¼‰...")
    densenet_yahoo = build_text_densenet(
        input_len=config.YAHOO_MAX_LEN,
        num_classes=yahoo_num_classes
    )
    densenet_yahoo_history = train_model(
        densenet_yahoo, X_train_yahoo, y_train_yahoo, X_val_yahoo, y_val_yahoo,
        num_classes=yahoo_num_classes, model_name="Text_DenseNet_Yahoo"
    )
    evaluate_model(densenet_yahoo, X_test_yahoo, y_test_yahoo, yahoo_num_classes, "Text_DenseNet", "Yahoo")
    plot_training_history(densenet_yahoo_history, "Text_DenseNet", "Yahoo")

    print("\n" + "="*50)
    print("6. è®­ç»ƒText DenseNetæ¨¡å‹ï¼ˆYelpæ•°æ®é›†ï¼‰...")
    densenet_yelp = build_text_densenet(
        input_len=config.YELP_MAX_LEN,
        num_classes=yelp_num_classes
    )
    densenet_yelp_history = train_model(
        densenet_yelp, X_train_yelp, y_train_yelp, X_val_yelp, y_val_yelp,
        num_classes=yelp_num_classes, model_name="Text_DenseNet_Yelp"
    )
    evaluate_model(densenet_yelp, X_test_yelp, y_test_yelp, yelp_num_classes, "Text_DenseNet", "Yelp")
    plot_training_history(densenet_yelp_history, "Text_DenseNet", "Yelp")

    print("\n" + "="*50)
    print("ğŸ‰ æ‰€æœ‰æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°å®Œæˆï¼")
    print(f"æ¨¡å‹æƒé‡ä¿å­˜åœ¨ï¼š{config.SAVE_MODEL_PATH}")
    print(f"å¯è§†åŒ–å›¾è¡¨ä¿å­˜åœ¨ï¼šå½“å‰é¡¹ç›®æ–‡ä»¶å¤¹ï¼ˆ.pngæ–‡ä»¶ï¼‰")

# æ‰§è¡Œä¸»å‡½æ•°
if __name__ == "__main__":

    main()
